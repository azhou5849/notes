\documentclass{article}
\usepackage{mathtools,amssymb,amsthm,enumitem,tikz}
\usepackage[letterpaper]{geometry}

\setlength{\parskip}{3pt plus 1pt minus 1pt}
\setlength{\parindent}{0pt}


\title{MA582 Homework UC}
\author{Alan Zhou}
\date{Due 2025-??-??}



\begin{document}

\maketitle


\section{Problem 1}

Suppose a parameter family has parameter $\theta = E(X^3)$, where $\theta$ is real. Devise an estimator of $\theta$ and show it is UC for $\theta$.

\begin{proof}[Solution]
Let $Y_n = X_n^3$ and consider
\begin{equation*}
\hat{\theta}_n = \frac{Y_1 + \cdots + Y_n}{n} = \frac{X_1^3 + \cdots + X_n^3}{n}.
\end{equation*}
This estimator is unbiased for $\theta$ because
\begin{equation*}
E(\hat{\theta}_n) = \frac{1}{n}\sum_{i = 1}^n E(Y_i) = \frac{1}{n}\sum_{i = 1}^n \theta = \theta.
\end{equation*}
This estimator is consistent for $\theta$ because the weak law of large numbers gives
\begin{equation*}
\hat{\theta}_n = \frac{Y_1 + \cdots + Y_n}{n}\stackrel{p}{\longrightarrow}\theta.
\end{equation*}
\end{proof}



\section{Problem 2}

We say the rv $X$ has the Weiner distribution with parameter $\theta > 0$ if $X$ has pdf $f(x) = 2x/\theta^2$ for $0 < x < \theta$ and $f(x) = 0$ elsewhere.

Consider the parameterized Weiner family $\{W(\theta) : \theta > 0\}$.

\begin{enumerate}[label=(\alph*)]
\item Let $Y_n$ be the maximum of the random sample of size $n$ from $W(\theta)$. Show that $Y_n$ is a consistent estimator of $\theta$.
\item Find the pdf of $Y_n$.
\item Show that $Y_n$ is NOT an unbiased estimator of $\theta$.
\item Show how to ``correct'' $Y_n$ here to make a new unbiased estimator $T_n$. Show $T_n$ is also consistent, hence UC.
\item Obtain the asymptotic distribution of $n(Y_n - \theta)$.
\item Obtain the asymptotic distribution of $n(T_n - \theta)$.
\end{enumerate}

\begin{proof}[Solution]
\begin{enumerate}[label=(\alph*)]
\item We must show $Y_n\stackrel{p}{\to}\theta$, meaning that for any $\epsilon > 0$, we have $P(\lvert Y_n - \theta\rvert\geq\epsilon)\to 0$ as $n\to\infty$. As $Y_n\leq\theta$, this probability equals $P(Y_n\leq\theta - \epsilon)$. Since $Y_n = \max(X_1, \ldots, X_n)$, the condition $Y_n\leq\theta - \epsilon$ holds precisely when $X_i\leq\theta - \epsilon$ for all $i$. As the $X_i$s are independent,
\begin{align*}
P(Y_n\leq\theta - \epsilon) &= \prod_{i = 1}^n P(X_i\leq\theta - \epsilon) \\
&= \left(\int_0^{\theta - \epsilon}\frac{2x}{\theta^2}\,dx\right)^n \\
&= \left(\frac{(\theta - \epsilon)^2}{\theta^2}\right)^n \\
&= \left(1 - \frac{\epsilon}{\theta}\right)^{2n}.
\end{align*}
As $n\to\infty$, this probability does indeed tend to $0$.
\item By the same reasoning as before, the cdf of $Y_n$ is
\begin{equation*}
F_{Y_n}(y) = P(Y_n\leq y) = \left(\int_0^y\frac{2x}{\theta^2}\,dx\right)^n = \frac{y^{2n}}{\theta^{2n}}
\end{equation*}
for $0 < y < \theta$, with $F_{Y_n}(y) = 0$ for $y\leq 0$ and $F_{Y_n}(y) = 1$ for $y\geq \theta$. The pdf of $Y_n$ is then
\begin{equation*}
f_{Y_n}(y) = F_{Y_n}'(y) = \frac{2n}{\theta^{2n}}\cdot y^{2n - 1}
\end{equation*}
when $0 < y < \theta$, with $f_{Y_n}(y) = 0$ for all other $y$.
\item The expected value of $Y_n$ is
\begin{align*}
E(Y_n) &= \int_0^{\theta} y\cdot f_{Y_n}(y)\,dy = \int_0^{\theta} \frac{2n}{\theta^{2n}} y^{2n}\,dy \\
&= \frac{2n}{(2n + 1)\theta^{2n}}\cdot\theta^{2n + 1} = \frac{2n}{2n + 1}\cdot\theta.
\end{align*}
In particular, this is not $\theta$ itself, so $Y_n$ is not an unbiased estimator for $\theta$.
\item If we define $T_n = \frac{2n + 1}{2n}Y_n$, then $E(T_n) = \frac{2n + 1}{2n}E(Y_n) = \theta$, so $T_n$ is an unbiased estimator for $\theta$. Also, since $\frac{2n + 1}{2n}\to 1$ and $Y_n\stackrel{p}{\to}\theta$ as $n\to\infty$, we have $T_n\stackrel{p}{\to} 1\cdot\theta = \theta$ as $n\to\infty$, so $T_n$ is consistent for $\theta$. Hence $T_n$ is UC for $\theta$.
\item 
\end{enumerate}
\end{proof}



\end{document}