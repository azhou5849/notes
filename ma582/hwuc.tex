\documentclass{article}
\usepackage{mathtools,amssymb,amsthm,enumitem,tikz}
\usepackage[letterpaper]{geometry}

\setlength{\parskip}{3pt plus 1pt minus 1pt}
\setlength{\parindent}{0pt}


\title{MA582 Homework UC}
\author{Alan Zhou}
\date{Due 2025-??-??}



\begin{document}

\maketitle


\section{Problem 1}

Suppose a parameter family has parameter $\theta = E(X^3)$, where $\theta$ is real. Devise an estimator of $\theta$ and show it is UC for $\theta$.

\begin{proof}[Solution]
Let $Y_n = X_n^3$ and consider
\begin{equation*}
\hat{\theta}_n = \frac{Y_1 + \cdots + Y_n}{n} = \frac{X_1^3 + \cdots + X_n^3}{n}.
\end{equation*}
This estimator is unbiased for $\theta$ because
\begin{equation*}
E(\hat{\theta}_n) = \frac{1}{n}\sum_{i = 1}^n E(Y_i) = \frac{1}{n}\sum_{i = 1}^n \theta = \theta.
\end{equation*}
This estimator is consistent for $\theta$ because the weak law of large numbers gives
\begin{equation*}
\hat{\theta}_n = \frac{Y_1 + \cdots + Y_n}{n}\stackrel{p}{\longrightarrow}\theta.
\end{equation*}
\end{proof}



\section{Problem 2}

We say the rv $X$ has the Weiner distribution with parameter $\theta > 0$ if $X$ has pdf $f(x) = 2x/\theta^2$ for $0 < x < \theta$ and $f(x) = 0$ elsewhere.

Consider the parameterized Weiner family $\{W(\theta) : \theta > 0\}$.

\begin{enumerate}[label=(\alph*)]
\item Let $Y_n$ be the maximum of the random sample of size $n$ from $W(\theta)$. Show that $Y_n$ is a consistent estimator of $\theta$.
\item Find the pdf of $Y_n$.
\item Show that $Y_n$ is NOT an unbiased estimator of $\theta$.
\item Show how to ``correct'' $Y_n$ here to make a new unbiased estimator $T_n$. Show $T_n$ is also consistent, hence UC.
\item Obtain the asymptotic distribution of $n(\theta - Y_n)$. [Originally: $n(Y_n - \theta)$]
\item Obtain the asymptotic distribution of $n(\theta - T_n)$. [Originally: $n(T_n - \theta)$]
\end{enumerate}

\begin{proof}[Solution]
If $X\sim W(\theta)$, then the cdf of $X$ satisfies $F_X(x) = 0$ for $x < 0$ and $F_X(x) = 1$ for $x\geq\theta$. In between, we calculate
\begin{equation*}
F_X(x) = \int_0^x\frac{2t}{\theta^2}\,dt = \frac{x^2}{\theta^2}.
\end{equation*}
Then, as discussed in class, when $Y_n = \max(X_1, \ldots, X_n)$ we have
\begin{equation*}
F_{Y_n}(y) = (F_X(y))^n = \begin{cases} 0 & y < 0, \\ (y/\theta)^{2n} & 0\leq y < \theta, \\ 1 & y\geq\theta. \end{cases}
\end{equation*}
\begin{enumerate}[label=(\alph*)]
\item Since $Y_n\leq\theta$, for any small $\epsilon > 0$,
\begin{equation*}
P(\lvert Y_n - \theta\rvert\geq\epsilon) = P(Y_n\leq\theta - \epsilon) = \left(\frac{\theta - \epsilon}{\theta}\right)^{2n} = \left(1 - \frac{\epsilon}{\theta}\right)^{2n}.
\end{equation*}
As $n\to\infty$, the right hand side tends to $0$. As $\epsilon > 0$ was arbitrary, $Y_n\stackrel{p}{\to}\theta$.
\item The pdf of $Y_n$ is
\begin{equation*}
f_{Y_n}(y) = F_{Y_n}'(y) = \frac{2n}{\theta^{2n}}\cdot y^{2n - 1}
\end{equation*}
when $0 < y < \theta$, with $f_{Y_n}(y) = 0$ for all other $y$.
\item The expected value of $Y_n$ is
\begin{align*}
E(Y_n) &= \int_0^{\theta} y\cdot f_{Y_n}(y)\,dy = \int_0^{\theta} \frac{2n}{\theta^{2n}} y^{2n}\,dy \\
&= \frac{2n}{(2n + 1)\theta^{2n}}\cdot\theta^{2n + 1} = \frac{2n}{2n + 1}\cdot\theta.
\end{align*}
In particular, this is not $\theta$ itself, so $Y_n$ is not an unbiased estimator for $\theta$.
\item If we define $T_n = \frac{2n + 1}{2n}Y_n$, then
\begin{equation*}
E(T_n) = \frac{2n + 1}{2n}E(Y_n) = \theta,
\end{equation*}
so $T_n$ is an unbiased estimator for $\theta$. Also, since $\frac{2n + 1}{2n}\to 1$ and $Y_n\stackrel{p}{\to}\theta$ as $n\to\infty$, another ``preservation of convergence in probability'' result gives us
\begin{equation*}
T_n = \frac{2n + 1}{2n}Y_n\stackrel{p}{\longrightarrow} 1\cdot\theta = \theta.
\end{equation*}
Thus $T_n$ is consistent for $\theta$ and hence UC for $\theta$.
\item When $t < 0$, we have $P(n(\theta - Y_n)\leq t) = 0$. Otherwise, for all $n$ sufficiently large ($n\theta > t$),
\begin{align*}
P(n(\theta - Y_n)\leq t) &= P\left(Y_n\geq\theta - \frac{t}{n}\right) = 1 - P\left(Y_n\leq\theta - \frac{t}{n}\right) \\
&= 1 - \left(\frac{\theta - t/n}{\theta}\right)^{2n} = 1 - \left(1 - \frac{t/\theta}{n}\right)^{2n}.
\end{align*}
As $n\to\infty$, this tends to $1 - e^{-2t/\theta}$, so the asymptotic distribution of $n(\theta - Y_n)$ has CDF
\begin{equation*}
t\longmapsto \begin{cases} 0 & t < 0, \\ 1 - e^{-2t/\theta} & t\geq 0. \end{cases}
\end{equation*}
This is the $\operatorname{Exp}(2/\theta)$ distribution.
\item We start by writing
\begin{equation*}
n(\theta - T_n) = n(\theta - Y_n) + n(Y_n - T_n) = n(\theta - Y_n) - \frac{1}{2}Y_n.
\end{equation*}
Since $n(\theta - Y_n)\stackrel{\mathcal{D}}{\to}\operatorname{Exp}(2/\theta)$ and $Y_n\stackrel{p}{\to}\theta$, Slutsky's lemma tells us that
\begin{equation*}
n(\theta - T_n)\stackrel{\mathcal{D}}{\longrightarrow}\operatorname{Exp}\left(\frac{2}{\theta}\right) - \frac{\theta}{2},
\end{equation*}
the $\operatorname{Exp}(2/\theta)$ distribution shifted a constant amount $\theta/2$ to the left.
\end{enumerate}
\end{proof}



\end{document}